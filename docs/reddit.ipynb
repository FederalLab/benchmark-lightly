{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a5d2e68",
   "metadata": {},
   "source": [
    "# Reddit\n",
    "\n",
    "We preprocess the Reddit data released by [pushshift.io](https://files.pushshift.io/reddit/) corresponding to December 2017. We perform the following operations:\n",
    "\n",
    "1. Unescape html symbols.\n",
    "2. Remove extraneous whitespaces.\n",
    "3. Remove non-ascii symbols.\n",
    "4. Replace URLS, reddit usernames and subreddit names with special tokens.\n",
    "5. Lowercase the text.\n",
    "6. Tokenize the text (using nltk's TweetTokenizer).\n",
    "\n",
    "We also remove users and comments that simple heuristics or preliminary inspections mark as bots; and remove users with less than 5 or more than 1000 comments (which account for less than 0.01% of users).\n",
    "We include the code for this preprocessing in the `preprocess` folder for reference, but host the preprocessed dataset [here](https://drive.google.com/file/d/1CXufUKXNpR7Pn8gUbIerZ1-qHz1KatHH/view?usp=sharing).\n",
    "We further preprocess the data to make it ready for our reference model (by splitting it into train/val/test sets and by creating sequences of 10 tokens for the LSTM) [here](https://drive.google.com/file/d/1lT1Z0N1weG-oA2PgC1Jak_WQ6h3bu7V_/view?usp=sharing).\n",
    "The vocabulary of the 10 thousand most common tokens in the data can be found [here](https://drive.google.com/file/d/1I-CRlfAeiriLmAyICrmlpPE5zWJX4TOY/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef7498",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c469fa0a",
   "metadata": {},
   "source": [
    "### Setup Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccedda33",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48905e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: nltk in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: click in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from nltk) (4.62.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from click->nltk) (4.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/densechen/miniconda3/envs/openfed/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07866811",
   "metadata": {},
   "source": [
    "#### Download preprocessed data from google driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac2947",
   "metadata": {},
   "source": [
    "To use our reference model, download the data [here](https://drive.google.com/file/d/1PwBpAEMYKNpnv64cQ2TIQfSc_vPbq3OQ/view?usp=sharing) into a `data` subfolder in `../benchmark/datasets/reddit/data`. This is a sub-sampled version of the complete data. Our reference implementation doesn't yet support training on the [complete dataset](https://drive.google.com/file/d/1lT1Z0N1weG-oA2PgC1Jak_WQ6h3bu7V_/view?usp=sharing), as it loads all given clients into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4dbda2",
   "metadata": {},
   "source": [
    "#### Download raw data and preprocess it manualy (Optional)\n",
    "\n",
    "Refer to [reddit](https://files.pushshift.io/reddit/comments/) for more details about dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20224e",
   "metadata": {},
   "source": [
    "- Open a terminal and run the following command to download and unzip dataset:\n",
    "\n",
    "    ```shell\n",
    "    FILENAME='RC_2005-12' # Select a file to download. must be consistent with L10 in `preprocess.py`\n",
    "    SUF_EXT='bz2' # `xz`, `bz2`, `zst`\n",
    "    mkdir -pv data/raw\n",
    "    cd data/raw\n",
    "    wget --no-check-certificate --no-proxy https://files.pushshift.io/reddit/comments/$FILENAME.$SUF_EXT\n",
    "\n",
    "    if [ $SUF_EXT = \"xz\" ]; then\n",
    "       # Install necessary tools to unxz the downloaded file\n",
    "       sudo apt-get install xz-utils\n",
    "       echo \"unxz $FILENAME.$SUF_EXT\"\n",
    "       unxz $FILENAME.$SUF_EXT\n",
    "    fi\n",
    "\n",
    "    if [ $SUF_EXT = \"bz2\" ]; then\n",
    "       echo \"bunzip2 $FILENAME.$SUF_EXT\"\n",
    "       bunzip2 $FILENAME.$SUF_EXT\n",
    "    fi\n",
    "\n",
    "    if [ $SUF_EXT = \"zst\" ]; then\n",
    "       echo \"tar $FILENAME.$SUF_EXT\"\n",
    "       tar -I zstd -xvf $FILENAME.$SUF_EXT\n",
    "    fi\n",
    "    ```\n",
    "\n",
    "- Preprocess:\n",
    "\n",
    "    ```shell\n",
    "    cd ../benchmark/datasets/reddit/preprocess\n",
    "    bash run_reddit.sh\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94539056",
   "metadata": {},
   "source": [
    "#### Build training vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c44b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading reddit_10_train.json\n",
      "counting reddit_10_train.json\n",
      "\n",
      "loading reddit_15_train.json\n",
      "counting reddit_15_train.json\n",
      "\n",
      "loading reddit_17_train.json\n",
      "counting reddit_17_train.json\n",
      "\n",
      "loading reddit_18_train.json\n",
      "counting reddit_18_train.json\n",
      "\n",
      "loading reddit_1_train.json\n",
      "counting reddit_1_train.json\n",
      "\n",
      "loading reddit_20_train.json\n",
      "counting reddit_20_train.json\n",
      "\n",
      "loading reddit_3_train.json\n",
      "counting reddit_3_train.json\n",
      "\n",
      "loading reddit_4_train.json\n",
      "counting reddit_4_train.json\n",
      "\n",
      "loading reddit_6_train.json\n",
      "counting reddit_6_train.json\n",
      "\n",
      "loading reddit_9_train.json\n",
      "counting reddit_9_train.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd ../benchmark/datasets/reddit && python build_vocab.py --data-dir ./data/train --target-dir vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06deaf3",
   "metadata": {},
   "source": [
    "### Valid dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.datasets.reddit import get_reddit\n",
    "dataset = get_reddit('../benchmark/datasets/reddit/data')\n",
    "print(dataset)\n",
    "x, y = dataset[0]\n",
    "print(x.shape, y.shape)\n",
    "print(f'vocab size: {dataset.vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94229c",
   "metadata": {},
   "source": [
    "## FedAvg, FedSGD, FedEla, FedProx, FedScaffold\n",
    "\n",
    "Run following commands in the root path of `benchmark-lightly`.\n",
    "\n",
    "```bash\n",
    "function cmd(){\n",
    "    fed_optim=$1\n",
    "\n",
    "    task_name=\"reddit\"\n",
    "    exp_name=${fed_optim}_${task_name}\n",
    "\n",
    "    # Delete cache file\n",
    "    rm -rf /tmp/${exp_name}.share\n",
    "    rm -rf /tmp/${exp_name}\n",
    "    rm -rf ./logs/${task_name}/${fed_optim}\n",
    "\n",
    "    # Run\n",
    "    python -m openfed.tools.launch --nproc_per_node 6  --logdir /tmp benchmark/run.py\\\n",
    "        --fed_init_method file:///tmp/${exp_name}.share\\\n",
    "        --task ${task_name}\\\n",
    "        --data_root benchmark/datasets/${task_name}/data\\\n",
    "        --epochs 1\\\n",
    "        --rounds 20\\\n",
    "        --act_clts 100\\\n",
    "        --tst_act_clts 100\\\n",
    "        --max_acg_step -1\\\n",
    "        --optim ${fed_optim}\\\n",
    "        --optim_args momentum:0.9 weight_decay:1e-4\\\n",
    "        --follower_lr 1e-1\\\n",
    "        --leader_lr 1.0\\\n",
    "        --bz 10\\\n",
    "        --gpu\\\n",
    "        --log_level SUCCESS\\\n",
    "        --log_dir logs\\\n",
    "        --exp_name ${exp_name}\\\n",
    "        --seed 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f9be7",
   "metadata": {},
   "source": [
    "### Run All\n",
    "\n",
    "```bash\n",
    "cmd 'fedavg'; cmd 'fedsgd'; cmd 'fedela'; cmd 'fedprox'; cmd 'fedscaffold'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836db013",
   "metadata": {},
   "source": [
    "## Plot Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f43619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from benchmark.utils.plot import plot\n",
    "\n",
    "task_name = \"reddit\"\n",
    "\n",
    "items = dict(\n",
    "    FedAvg=f'../logs/{task_name}/fedavg_{task_name}/{task_name}.json',\n",
    "    FedSgd=f'../logs/{task_name}/fedsgd_{task_name}/{task_name}.json',\n",
    "    FedEla=f'../logs/{task_name}/fedela_{task_name}/{task_name}.json',\n",
    "    FedProx=f'../logs/{task_name}/fedprox_{task_name}/{task_name}.json',\n",
    "    FedScaffold=f'../logs/{task_name}/fedscaffold_{task_name}/{task_name}.json',\n",
    ")\n",
    "\n",
    "files = items.values()\n",
    "labels = items.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6bdc12",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8635c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    files=files,\n",
    "    labels=labels,\n",
    "    attributes=\"accuracy\",\n",
    "    mode='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc230c",
   "metadata": {},
   "source": [
    "### Train Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    files=files,\n",
    "    labels=labels,\n",
    "    attributes=\"loss\",\n",
    "    mode=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2c74f",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    files=files,\n",
    "    labels=labels,\n",
    "    attributes=\"accuracy\",\n",
    "    mode=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a924f",
   "metadata": {},
   "source": [
    "### Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf06b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    files=files,\n",
    "    labels=labels,\n",
    "    attributes=\"loss\",\n",
    "    mode='test'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
